#  Cross-Modal Transformer for Emotion Recognition

**Multimodal Architecture Using EEG and Eye-Tracking Features (SEED-V)**
## Model Architecture

![Cross-Modal Transformer Architecture](https://github.com/user-attachments/assets/fd345259-9921-49fa-aa3f-5948e64fab27)

*Model architecture diagram by torchgreedy (Last updated: 2025-08-07)*

![Python](https://img.shields.io/badge/python-3.10-blue) 
![PyTorch](https://img.shields.io/badge/PyTorch-1.10+-ee4c2c?logo=pytorch&logoColor=white)
![License](https://img.shields.io/badge/license-MIT-green)

##  Overview

This project presents a **Transformer-based multimodal architecture** for classifying human emotions using EEG and eye-tracking signals from the [SEED-V dataset](https://bcmi.sjtu.edu.cn/home/seed/seed-v.html).  
It incorporates **novel attention mechanisms**, **domain adaptation**, and **interpretable gating** to outperform previous benchmarks under rigorous evaluation.

### Core Features

1. **Positional Encoding**: Sinusoidal encodings enable the model to understand temporal relationships in sequential data.

2. **Feature Importance Module (FIM)**: Learns modality-specific importance weights to adaptively emphasize stronger signals.

3. **Cross-Modal Attention**: Bidirectional attention mechanism allowing EEG features to attend to eye movement features and vice versa.

4. **Self-Attention Transformer Encoder**: Processes each modality to capture intra-modal temporal dynamics.

5. **Global Pooling and Fusion**: Integrates information from both modalities into a unified representation.

6. **Domain Adaptation Layer**: Employs gradient reversal to learn subject-invariant representations, improving generalization.

7. **Classification Head**: Multi-layer perceptron with GELU activations for final emotion classification.

For detailed mathematical formulations and implementation specifics, see our [technical documentation](docs/technical_overview.md).

> **Achieved Accuracy: 75.42% on SEED-V (LOSO evaluation)**
## Model Performance

Our Cross-Modal Transformer achieves **75.42% Â± 11.15%** mean accuracy on the SEED-V dataset using rigorous Leave-One-Subject-Out (LOSO) cross-validation.

### Per-Subject Performance

| Subject ID | Accuracy |
|------------|----------|
| Subject 1  | 88.89%   |
| Subject 2  | 84.44%   |
| Subject 3  | 82.22%   |
| Subject 4  | 60.00%   |
| Subject 5  | 75.56%   |
| Subject 6  | 84.44%   |
| Subject 7  | 64.44%   |
| Subject 8  | 86.67%   |
| Subject 9  | 55.56%   |
| Subject 10 | 64.44%   |
| Subject 11 | 75.56%   |
| Subject 12 | 75.56%   |
| Subject 13 | 80.00%   |
| Subject 14 | 57.78%   |
| Subject 15 | 80.00%   |
| Subject 16 | 91.11%   |
| **Average**| **75.42%**|

The variance in performance across subjects (standard deviation: 11.15%) highlights the challenge of cross-subject generalization in physiological emotion recognition, which our domain adaptation approach helps address.

## ğŸ“ Project Structure

```
â”œâ”€â”€ Cross-Modal-Transformer-for-Robust-Emotion-Recognition/
â”‚   â”œâ”€â”€ checkpoints/                    # 16 model files
â”‚   â”‚   â”œâ”€â”€ model_fold_1.pth            
â”‚   â”‚   â”œâ”€â”€ model_fold_2.pth
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ model_fold_16.pth
â”‚   â”œâ”€â”€ docs/                           # Technical documentation
â”‚   â”‚   â”œâ”€â”€ technical_overview.md       # Overall architecture overview
â”‚   â”‚   â””â”€â”€ modules/                    # Detailed module descriptions
â”‚   â”‚       â”œâ”€â”€ classification_head.md  # Classification component details
â”‚   â”‚       â”œâ”€â”€ cross_modal_attention.md# Bidirectional attention mechanism
â”‚   â”‚       â”œâ”€â”€ domain_adaptation.md    # Subject-invariant learning approach
â”‚   â”‚       â”œâ”€â”€ feature_importance.md   # Adaptive modality weighting
â”‚   â”‚       â”œâ”€â”€ global_pooling.md       # Feature fusion methods
â”‚   â”‚       â”œâ”€â”€ linear_projection.md    # Input dimension alignment
â”‚   â”‚       â”œâ”€â”€ positional_encoding.md  # Temporal information encoding
â”‚   â”‚       â””â”€â”€ self_attention.md       # Intra-modal attention mechanism
â”‚   â”œâ”€â”€ SEED-V/
â”‚   â”‚   â”œâ”€â”€ EEG_DE_features/            # EEG differential entropy features data, just reference only
â”‚   â”‚   â””â”€â”€ Eye_movement_features/      # Eye movement features data, just reference only
â”‚   â”œâ”€â”€ interpretability_results/
â”‚   â”‚   â”œâ”€â”€ attention_analysis/
â”‚   â”‚   â”‚   â”œâ”€â”€ attention_heatmaps.png
â”‚   â”‚   â”‚   â””â”€â”€ temporal_attention_patterns.png
â”‚   â”‚   â”œâ”€â”€ confidence_analysis/
â”‚   â”‚   â”‚   â””â”€â”€ confidence_analysis.png
â”‚   â”‚   â”œâ”€â”€ emotion_patterns/
â”‚   â”‚   â”‚   â””â”€â”€ emotion_specific_attention.png
â”‚   â”‚   â”œâ”€â”€ subject_analysis/
â”‚   â”‚   â”‚   â””â”€â”€ subject_variability_analysis.png
â”‚   â”‚   â””â”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ model.py                    # Main multimodal Transformer model
â”‚   â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”‚   â”œâ”€â”€ projections.py          # Modality projection layers
â”‚   â”‚   â”‚   â”œâ”€â”€ attention.py            # Cross-modal and self-attention modules
â”‚   â”‚   â”‚   â”œâ”€â”€ grl.py                  # Domain adaptation (Gradient Reversal Layer)
â”‚   â”‚   â”‚   â”œâ”€â”€ fusion.py               # Gating and final fusion logic
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py             # Module exports
â”‚   â”‚   â”œâ”€â”€ dataset.py                  # SEED-V preprocessing & Dataloader
â”‚   â”‚   â”œâ”€â”€ utils.py                    # Masking, normalization, evaluation helpers
â”‚   â”‚   â””â”€â”€ __init__.py                 # Package exports
â”‚   â”œâ”€â”€ train.py                        # Training script with LOSO evaluation
â”‚   â”œâ”€â”€ evaluate.py                     # Standalone evaluation
â”‚   â”œâ”€â”€ config.yaml                     # Model and training hyperparameters
â”‚   â”œâ”€â”€ requirements.txt                # Project dependencies
â”‚   â”œâ”€â”€ LICENSE                         # Project license
â”‚   â””â”€â”€ README.md                       # Project documentation
```

## ğŸ› ï¸ Installation

1. Clone the repository:
```bash
git clone https://github.com/torchgreedy/Cross-Modal-Transformer-for-Robust-Emotion-Recognition.git
cd Cross-Modal-Transformer-for-Robust-Emotion-Recognition
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

##  Dataset

This project uses the SEED-V dataset containing EEG and eye-tracking data for emotion recognition. The dataset includes:

- **5 emotion categories**: happy, sad, fear, disgust, neutral
- **16 subjects** with 3 sessions each
- **62-channel EEG data** and **eye movement features**

You'll need to request access to the dataset from the [BCMI lab](https://bcmi.sjtu.edu.cn/home/seed/seed-v.html).

##  Usage

### Training

To train the model with Leave-One-Subject-Out cross-validation:

```bash
python train.py --config config.yaml
```

### Evaluation

To evaluate a trained model:

```bash
python evaluate.py --model_path checkpoints/model_fold_1.pth --subject_id 1
```

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.
