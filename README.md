# Cross-Modal-Transformer-for-Robust-Emotion-Recognition

# ğŸ§  Cross-Modal Transformer for Emotion Recognition  
**Multimodal Architecture Using EEG and Eye-Tracking Features (SEED-V)**

![Python](https://img.shields.io/badge/python-3.10-blue) ![License](https://img.shields.io/badge/license-MIT-green)

---

## ğŸš€ Overview

This project presents a **Transformer-based multimodal architecture** for classifying human emotions using EEG and eye-tracking signals from the [SEED-V dataset](https://bcmi.sjtu.edu.cn/home/seed/seed-v.html).  
It incorporates **novel attention mechanisms**, **domain adaptation**, and **interpretable gating** to outperform previous benchmarks under rigorous evaluation.

### Core Features

-  **Modality-Specific Linear Projections**
-  **Bidirectional Cross-Modal Attention**
-  **Modality Importance Weighting via Gating Units**
-  **Domain Adaptation with Gradient Reversal Layer (GRL)**
-  **Subject-Specific Normalization**
-  **LOSO Cross-Validation (Leave-One-Subject-Out)**

>  **Achieved Accuracy: 75.42% on SEED-V (LOSO evaluation)**

---

## ğŸ“ Project Structure


â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ model.py # Main multimodal Transformer model
â”‚ â”œâ”€â”€ modules/
â”‚ â”‚ â”œâ”€â”€ projections.py # Modality projection layers
â”‚ â”‚ â”œâ”€â”€ attention.py # Cross-modal and self-attention modules
â”‚ â”‚ â”œâ”€â”€ grl.py # Domain adaptation (Gradient Reversal Layer)
â”‚ â”‚ â”œâ”€â”€ fusion.py # Gating and final fusion logic
â”‚ â”œâ”€â”€ dataset.py # SEED-V preprocessing & Dataloader
â”‚ â””â”€â”€ utils.py # Masking, normalization, evaluation helpers
â”œâ”€â”€ train.py # Training script with LOSO evaluation
â”œâ”€â”€ evaluate.py # Standalone evaluation
â”œâ”€â”€ config.yaml # Model and training hyperparameters
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md



